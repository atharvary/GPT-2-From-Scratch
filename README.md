<h1 align="center">ğŸ“š LLMs from Scratch</h1>
<p align="center"><i>A hands-on journey into the inner workings of Large Language Models</i></p>

---

## ğŸ“ Overview  
I built a **Large Language Model (LLM)** completely from scratch using **PyTorch**, following the GPT architecture.  
This project helped me move from seeing LLMs as a mysterious *black box* to understanding them as a *glass box* â€” clear, interpretable, and deeply fascinating.  

Along the way, I learned how each part fits together â€” from tokenization to self-attention â€” and how modern LLMs actually generate coherent text.  

---

## ğŸš€ Features  
- Implemented the **Transformer architecture** from scratch  
- Built the **Multi-Head Self-Attention mechanism**  
- Created **Token Embeddings** and **Positional Embeddings**  
- Followed the **GPT architecture** closely  
- Trained on a real text dataset and later **imported GPT-2 weights** for performance boost  

---

## ğŸ“‚ Dataset  
- **The Verdict** by *Edith Wharton*  
- Used for training and experimentation  

---

## ğŸ“„ Learning Notes  
The code file itself is filled with self-explanatory notes so that with each run of the code cell, we are clear about what's happening.

---

## ğŸ“š References  
- **[Vizuara YouTube Channel â€“ Building LLMs from Scratch](https://www.youtube.com/@vizuara)**  
  One of the most detailed and comprehensive free courses for understanding and implementing LLMs.
